### 综合笔记：从概率到监督学习（包括英文翻译）

这份笔记详细解释了从概率论基础到监督学习、泛化与过拟合，以及强化学习和无监督学习的核心概念和应用实例。目标是帮助你理解每一步的原理和应用。

---

#### 1. 概率基础（Probability Basics）

**概率分布（Probability Distribution）**：
- 概率分布 \( p = \langle 0.6, 0.4 \rangle \) 表示两个事件发生的概率分别为0.6和0.4。
  - 例子：一个不公平的硬币，正面朝上的概率为0.6，反面朝上的概率为0.4。
  - **英文**：Probability distribution \( p = \langle 0.6, 0.4 \rangle \) represents the probabilities of two events occurring, 0.6 and 0.4 respectively.
  - **Example**: An unfair coin with a 0.6 probability of landing heads and 0.4 probability of landing tails.

**信息熵（Information Entropy）**：
- 信息熵衡量一个概率分布的不确定性。公式为：
  \[
  H(p) = -\sum_{i=1}^{n} p_i \log_2 p_i
  \]
  - 例子：对于概率分布 \( p = \langle 0.5, 0.5 \rangle \)，信息熵为：
    \[
    H(p) = - (0.5 \log_2 0.5 + 0.5 \log_2 0.5) = 1 \text{ bit}
    \]
  - **英文**：Information entropy measures the uncertainty of a probability distribution. The formula is:
    \[
    H(p) = -\sum_{i=1}^{n} p_i \log_2 p_i
    \]
  - **Example**: For the probability distribution \( p = \langle 0.5, 0.5 \rangle \), the information entropy is:
    \[
    H(p) = - (0.5 \log_2 0.5 + 0.5 \log_2 0.5) = 1 \text{ bit}
    \]

**KL散度（KL-Divergence）**：
- KL散度用于衡量两个概率分布之间的差异。
  - 前向KL散度 \( D_{KL}(P || Q) \) 公式为：
    \[
    D_{KL}(P || Q) = \sum_{i=1}^{n} P_i (\log_2 P_i - \log_2 Q_i)
    \]
    - 例子：如果 \( P = \langle 0.6, 0.4 \rangle \) 和 \( Q = \langle 0.5, 0.5 \rangle \)，计算前向KL散度。
  - **英文**：KL-Divergence measures the difference between two probability distributions.
    - Forward KL-Divergence \( D_{KL}(P || Q) \) is defined as:
      \[
      D_{KL}(P || Q) = \sum_{i=1}^{n} P_i (\log_2 P_i - \log_2 Q_i)
      \]
    - **Example**: If \( P = \langle 0.6, 0.4 \rangle \) and \( Q = \langle 0.5, 0.5 \rangle \), calculate the forward KL-Divergence.

  - 反向KL散度 \( D_{KL}(Q || P) \) 公式为：
    \[
    D_{KL}(Q || P) = \sum_{i=1}^{n} Q_i (\log_2 Q_i - \log_2 P_i)
    \]
    - **英文**：Reverse KL-Divergence \( D_{KL}(Q || P) \) is defined as:
      \[
      D_{KL}(Q || P) = \sum_{i=1}^{n} Q_i (\log_2 Q_i - \log_2 P_i)
      \]

**高斯分布（Gaussian Distribution）**：
- 高斯分布用于建模连续数据。
  - 单变量高斯分布的公式：
    \[
    P_{\mu, \sigma}(x) = \frac{1}{\sqrt{2\pi\sigma}} \exp\left( -\frac{(x - \mu)^2}{2\sigma^2} \right)
    \]
    - 例子：均值为0，标准差为1的标准正态分布。
  - **英文**：Gaussian distribution models continuous data.
    - The formula for univariate Gaussian distribution is:
      \[
      P_{\mu, \sigma}(x) = \frac{1}{\sqrt{2\pi\sigma}} \exp\left( -\frac{(x - \mu)^2}{2\sigma^2} \right)
      \]
    - **Example**: Standard normal distribution with mean 0 and standard deviation 1.

  - 多变量高斯分布的公式：
    \[
    P_{\mu, \Sigma}(x) = \frac{1}{\sqrt{(2\pi)^d |\Sigma|}} \exp\left( -\frac{1}{2} (x - \mu)^\top \Sigma^{-1} (x - \mu) \right)
    \]
    - 例子：多个变量之间有协方差的高斯分布。
  - **英文**：Multivariate Gaussian distribution is defined as:
    \[
    P_{\mu, \Sigma}(x) = \frac{1}{\sqrt{(2\pi)^d |\Sigma|}} \exp\left( -\frac{1}{2} (x - \mu)^\top \Sigma^{-1} (x - \mu) \right)
    \]
    - **Example**: Gaussian distribution with covariance among multiple variables.

---

#### 2. 信息熵和Huffman编码（Information Entropy and Huffman Coding）

**信息熵（Information Entropy）**：
- 信息熵度量离散概率分布中的不确定性。公式为：
  \[
  H(p) = -\sum_{i=1}^{n} p_i \log_2 p_i
  \]
  - 例子：对于概率分布 \( p = \langle 0.5, 0.25, 0.25 \rangle \)，信息熵为：
    \[
    H(p) = - (0.5 \log_2 0.5 + 0.25 \log_2 0.25 + 0.25 \log_2 0.25) = 1.5 \text{ bits}
    \]
  - **英文**：Information entropy measures the uncertainty in a discrete probability distribution. The formula is:
    \[
    H(p) = -\sum_{i=1}^{n} p_i \log_2 p_i
    \]
    - **Example**: For the probability distribution \( p = \langle 0.5, 0.25, 0.25 \rangle \), the information entropy is:
      \[
      H(p) = - (0.5 \log_2 0.5 + 0.25 \log_2 0.25 + 0.25 \log_2 0.25) = 1.5 \text{ bits}
      \]

**Huffman编码（Huffman Coding）**：
- Huffman编码是一种无损数据压缩算法，通过字符的出现频率进行高效编码。
  - 例子：对于概率分布 \( p = \langle 0.5, 0.25, 0.25 \rangle \)，可以用A=0，B=10，C=11进行编码。
  - **英文**：Huffman coding is a lossless data compression algorithm that efficiently encodes characters based on their frequencies.
    - **Example**: For the probability distribution \( p = \langle 0.5, 0.25, 0.25 \rangle \), we can encode A=0, B=10, C=11.

---

#### 3. 曲线拟合和过拟合（Curve Fitting and Overfitting）

**曲线拟合（Curve Fitting）**：
- 监督学习的目标是基于输入属性准确预测测试集中的目标值。
  - 例子：将曲线拟合到一组数据点上，可以选择直线、抛物线或高次多项式。
  - **英文**：The goal of supervised learning is to accurately predict the target value for all items in the test set based on input attributes.
    - **Example**: Fitting a curve to a set of data points, where you might choose a straight line, parabola, or higher-order polynomial.

**过拟合（Overfitting）**：
- 过拟合是指模型在训练集上表现很好，但在测试集上表现很差。
  - 例子：使用高次多项式拟合数据点，虽然在训练数据上表现良好，但在新数据上表现可能较差。
 

**过拟合（Overfitting）**：
- 过拟合是指模型在训练集上表现很好，但在测试集上表现很差。
  - 例子：使用高次多项式拟合数据点，虽然在训练数据上表现良好，但在新数据上表现可能较差。
  - **英文**：Overfitting occurs when a model performs very well on the training set but poorly on the test set.
    - **Example**: Fitting a high-order polynomial to data points may perform well on the training data but likely performs poorly on new data.

**泛化（Generalisation）**：
- 泛化是指模型在新数据上的表现，即在训练集和测试集上都表现良好。
  - 例子：选择相对简单的模型，如抛物线，使其在新数据上也能表现良好。
  - **英文**：Generalisation refers to a model's performance on new data, performing well on both the training set and the test set.
    - **Example**: Choosing a relatively simple model, such as a parabola, that performs well on new data.

**奥卡姆剃刀原则（Ockham's Razor）**：
- 最可能的假设是与数据一致的最简单假设。
  - 例子：选择中间复杂度的曲线既能拟合数据又不会过于复杂。
  - **英文**：The simplest hypothesis that fits the data is most likely.
    - **Example**: Choosing a medium complexity curve that fits the data well without being overly complex.

---

#### 4. 训练、验证和测试误差（Training, Validation, and Test Errors）

**避免过拟合的方法**：
1. **正则化（Regularization）**：
   - **L1正则化（L1 Regularization）**：加入参数绝对值的和作为惩罚项。
   - **L2正则化（L2 Regularization）**：加入参数平方和作为惩罚项。
   - **英文**：L1 and L2 regularization add penalties to the model parameters to prevent overfitting.
2. **交叉验证（Cross-Validation）**：
   - 将数据分为训练集、验证集和测试集。
   - **英文**：Cross-validation splits the data into training, validation, and test sets to prevent overfitting.
3. **数据增强（Data Augmentation）**：
   - 通过对训练数据进行随机变换增加数据集的多样性。
   - **英文**：Data augmentation increases the diversity of the training set through random transformations.
4. **提前停止（Early Stopping）**：
   - 在验证集误差开始上升时停止训练。
   - **英文**：Early stopping halts training when the validation error begins to rise.
5. **Dropout**：
   - 在每个小批量训练时随机选择一部分节点不用于训练，以提高网络的泛化能力。
   - **英文**：Dropout randomly excludes nodes during training to improve generalization.

---

#### 5. 强化学习（Reinforcement Learning）

**定义**：
- 通过与环境的交互来学习策略，以最大化累积奖励。
  - **英文**：Reinforcement Learning (RL) learns policies through interactions with the environment to maximize cumulative rewards.

**关键元素**：
1. **代理（Agent）**：做出动作的学习者。
   - **英文**：Agent - the learner or decision-maker.
2. **环境（Environment）**：代理与之交互的外部系统。
   - **英文**：Environment - the external system the agent interacts with.
3. **状态（State, \( s \)）**：环境在某一时刻的情况。
   - **英文**：State - the situation of the environment at a certain time.
4. **动作（Action, \( a \)）**：代理在某一状态下可以采取的行为。
   - **英文**：Action - the behavior the agent can take in a state.
5. **奖励（Reward, \( r \)）**：代理采取某一动作后环境给出的反馈信号。
   - **英文**：Reward - the feedback signal from the environment after an action.
6. **策略（Policy, \( \pi \)）**：代理选择动作的规则或函数。
   - **英文**：Policy - the rule or function by which the agent chooses actions.
7. **值函数（Value Function, \( V(s) \)）**：评估在状态 \( s \) 下的长期奖励期望。
   - **英文**：Value Function - evaluates the expected long-term rewards in a state.

**例子**：
- **Q-Learning**：一种常用的强化学习算法，用于寻找最优策略。
  - **英文**：Q-Learning is a common RL algorithm used to find the optimal policy.

---

#### 6. 无监督学习（Unsupervised Learning）

**定义**：
- 在没有标签数据的情况下进行训练，目标是发现数据中的隐藏模式或结构。
  - **英文**：Unsupervised Learning trains on data without labels, aiming to discover hidden patterns or structures.

**常见算法**：
1. **聚类（Clustering）**：
   - 例子：K-means 聚类算法将数据点分为 \( k \) 个聚类，使每个数据点属于离它最近的聚类中心。
   - **英文**：Clustering algorithms group data points into clusters based on similarity.
     - **Example**: K-means clustering algorithm divides data points into \( k \) clusters, assigning each data point to the nearest cluster center.
2. **降维（Dimensionality Reduction）**：
   - 例子：主成分分析（PCA）通过减少特征数量来简化数据。
   - **英文**：Dimensionality reduction simplifies data by reducing the number of features.
     - **Example**: Principal Component Analysis (PCA) reduces the number of features to simplify data analysis.

**K-means聚类（K-means Clustering）**：
- K-means是一种常用的无监督学习算法，用于将数据点分为 \( k \) 个聚类，使每个数据点属于离它最近的聚类中心。
  - **英文**：K-means is a popular unsupervised learning algorithm that groups data points into \( k \) clusters, assigning each data point to the nearest cluster center.
  - 例子：将二维数据点分为三类，通过K-means算法确定聚类中心并分配数据点。
  - **Example**: Dividing two-dimensional data points into three clusters using the K-means algorithm.

**Types of Learning**：
➛ Supervised Learning：agent is presented with examples of inputs and their target outputs
➛ Reinforcement Learning：agent is not presented with target outputs, but is given a reward signal,which it aims to maximize
➛ Unsupervised Learning：agent is only presented with the inputs themselves, and aims to find structure in these inputs

监督学习（Supervised Learning）：代理（算法）被呈现输入及其目标输出的例子。
强化学习（Reinforcement Learning）：代理没有被呈现目标输出，但会收到一个旨在最大化的奖励信号。
无监督学习（Unsupervised Learning）：代理只被呈现输入本身，并旨在找到这些输入的结构

Supervised Learning：
we have a training set and a test set, each consisting of a set of items;
for each item, a number of input attributes and a target value are specified.
➛ the aim is to predict the target value, based on the input attributes.
➛ agent is presented with the input and target output for each item in the
training set; it must then predict the output for each item in the test set
➛ various learning paradigms are available:
→ Neural Network
→ Decision Tree
→ Support Vector Machine, etc.

![Alt text](image.png)

Ways to Avoid Overfitting in Neural Netorks
➛ Limit the number of hidden nodes or connections
➛ Limit the number of training epochs (weight updates)
➛ Dropout
➛ Weight Decay (Week 3)
➛ Data Augmentation (Week 4)
➛ Regularization

![Alt text](image-1.png)

横轴：权重更新的次数、隐藏节点数量、dropout等
竖轴：误差大小，预测输出和实际输出之间的差异度量

assumption：图表显示了过拟合的情况，因为随着模型在训练次数上的增加，模型逐渐学会了减少在训练集的预测错误，而且验证错误最终停止下降并且还有上升，表明模型对训练数据过度优化，对新数据不能有效泛化，这就是过拟合的表现
这种观察帮助确定如何调整模型参数，例如通过提早停止训练、调整网络结构、应用正则化技术如Dropout等，来改善模型对新数据的泛化能力

![Alt text](image-2.png)

Dropout (7.12)
➛ When training is finished and the network is deployed, all nodes are used,
but the activation of each node is multiplied by the probability of “keeping” the
node during training.
➛ Thus, the activation received by each node is the average value of the
activation it would have received during training.
➛ Dropout forces the network to achieve redundancy because it must deal with
situations where some features are missing.
➛ Another way to view dropout is that it implicitly (and efficiently) simulates an
ensemble of different architectures.
